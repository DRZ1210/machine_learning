## cifar-10 进行图片分类

### 一 前言

`cifar-10` 数据包含60000张 32 x 32 的彩色图片，其中训练集中有 5000张，测试集中有 10000张， cifar-10中一共有10类图片，每种图片6000张。

关于数据下载，在网上有很多链接，但是因为需要下载 `cifar10.py` 和 `cifar10_input.py` 文件，现在 github.com中的tensorflow/models.git 中找不到 `cifar-10` 相关的文件，这主要是版本的问题，在下载时，需要选定相关的版本，本人选用的是 v1.13.0 版本的：[相关类文件下载链接](https://github.com/tensorflow/models/tree/v1.13.0) 

下载好相关.py文件之后，最好修改 cifar10.py 文件中文件下载的路径，原路经是在 根目录的tmp文件夹下面 /tmp，这样每次关机后，/tmp文件下的东西被删除，很不方便。有 `cifar10.py` 和 `cifar10_input.py` 之后数据集不需要自己额外下载，一开始担心 “墙” 的原因，担心下载不成功，但是可以成功下载。

目前基于 cifar-10数据集，最好的成果，其错误率达到了 3.5%，超过了人类的识别能力。



### 二 网络结构

定义: batch_size = 128, max_step = 3000

1. 在训练模型之前，首先进行了数据增强 (Data Argumentation)，对图片进行了诸如 水平翻转、设置亮度和对比度、随机裁剪24 x 24 大小的图片、数据标准化等操作，数据增强比较耗费CPU时间，采用线程池实现，线程池中16个线程。
2. 首先每次从 训练集中拿出 batch_size 张图片，所以模型的输入是： [128, 24, 24, 3]
3. **第一层 卷积**，卷积核为 [5, 5, 3, 64], strides = [1, 1, 1, 1], padding='SAME'，输出为[128, 24, 24, 64]，加上bias。
4. 采用最大池化，池化核为[1, 3, 3, 1], strides = [1, 2, 2, 1], padding='SAME'，输出为[128, 12, 12, 64]
5. LRN层，局部归一化，增强模型的范化能力，现在使用不多，较多使用的是 dropout, LRN 对于 Relu 这些没有上限的激活函数比较有用，但是对于 sigmoid 这些有确定边界值的激活函数的作用不大。
6. **第二层 卷积**，操作和第一层卷积操作类似，注意上一层的输出，作为本层的输入，所以通道数为64，并且本次卷积先进行 LRN 操作，最后才进行max_pool 操作。本层的输出为[128, 6, 6, 64]
7. **第三层 全连接层**， 首先将第二层卷积的输出做 flatten 扁平化操作，形状变为[128, 2304]，这一层的输出结点的个数为384，所以中间的全连接权重矩阵的形状为[2304, 384], 之后加上bias 偏移量，输出为[128, 384]
8. **第四层 全连接层**，操作就是普通的全连接操作，输出结点的数量为192，所以全连接权重矩阵形状为[384, 192]，该层输出为 [128, 192]。
9. **第五层 全连接层**，同上，输出结点数量为 10， 全连接矩阵的形状为[192, 10]， 该层输出的形状为 [128, 10]，在mnist中，最后一层采用了 softmax 操作，但是这里没有，我们在计算 loss的时候会计算 。



### 三 损失函数

损失函数这里不仅仅是 预计和 实际之间的区别，为了防止特征过多导致过拟合，我们对每一个 权重 weight 进行计算了 L2 loss，关于 L1 和 L2 正则化，这里就不多说，L1会制造稀疏的特征，L2会让特征的权重不是很大。

不管是Conv 中的卷积核 还是 全连接层中的权重矩阵，创建时候，都会有一个对应的loss，被加入到一个 名为 `lossess`的列表中（其实主要是全连接层中的权重，全连接层中的权重矩阵确实是很大，所以很有惩罚的必要，对于Conv中的卷积核，这里设定的weight loss 为0）

对于预测和实际之间的差距，此处采用`cross entropy `交叉熵损失函数，在训练时，每训练一个 batch 的数据，就可以得到一个 交叉熵，我们将这个loss 加入到 losses 列表中，之后返回 sum (losses)

所以训练每次计算 loss 都是加和 lesses 中的所有的loss, 这些loss 有些是训练中预测和实际的交叉熵，一部分是权重的损失。



### 四 Optimizer

采用 Adam.Optimer，学习率为1e-3 （若采用学习速率衰减的 SGD随机梯度下降 的优化方法，模型的最后的准确率会更好）

---

### 五 输出

训练之后，模型在测试集上的准确率为 89.468%，增加模型的训练次数 （从之前的3000增加到4000）模型的准确率为 91.975%。



主要的收获：采用L2 正则，将权重的loss 作为整体损失的一部分、再次熟悉了网络整个的搭建流程、以及LRN 局部归一化操作。

![image-20201028124956735](/home/dengruizhi/.config/Typora/typora-user-images/image-20201028124956735.png)



![image-20201028125008458](/home/dengruizhi/.config/Typora/typora-user-images/image-20201028125008458.png)







