## VGGNet网络

### 一 前言

&emsp;&emsp;VGGNet网络是由 牛津大学计算机视觉组和Google DeepMind小组共同实现的卷积神经网络，VGGNet 按照网络的层数分为A、B、C、D、E五个版本，其中人们广泛熟悉的VGGNet-16和VGGNet-19就是D和E这两个版本，其中的16指的就是weight的个数，或者说就是卷积操作和全连接操作的层数 （池化层不算，因为池化层没有需要训练的权重）本次实现的是VGGNet-16 的版本，最后同样没有实际训练网络，下载数据集训练不现实，所以只是随机创建了一些数据并跑了一下每一轮batch_size所需的时间。

&emsp;&emsp;VGGNet 网络在2014年的ILSVRP大赛的亚军，败于GoogleNet。

## 二 收获

&emsp;&emsp;对于整体的网络搭建，并没有什么新奇，只是 卷积层、池化层、全连接层这些经常使用的操作，各自创建了实现的函数，使用时仅需要传入相应的参数，以卷积操作为例，需要传入：输入的tensor，卷积核的形状、个数、步长，padding的类型，该层的名字等。

&emsp;&emsp;VGGNet网络中存在多层卷积操作串连的情况（上一个卷积操作的输出作为后一个卷积操作的输入）这样的操作，其实就是多个卷积核的堆叠，<font color = red>**VGGNet中存在经常出现多个 完全一致的3x3卷积层堆叠在一起，这显然是很优秀的设计**</font>。若我们假设卷积核移动的步长为1，则两个3 x 3卷积核和一个5 x 5大小的卷积核的感受野的大小是一致的，同理三个 3 x 3卷积核堆叠和一个7 x7 卷积核是一致的，多个卷积核的堆叠效果想比较后者是好很多的，<font color = red>**前者参数的个数为3 * 3 * 3 = 27，但是后者 1 * 7 * 7 = 49，从参数个数来说，前者更少，前者学习能力同样强于后者，前者会用到三次 Relu 激活函数，后者只用到了一次，所以前者对于非线性的映射关系更好一些，学习能力自然也就更好**</font>。



卷积、池化、全连接部分，分别封装为一个函数，避免代码冗余。

多个卷积层串连，即进行卷积核的堆叠，效果更好（参数数量和学习能力都更好）

当然不管卷积部分参数有多少，相比较于全连接层，依旧还是少得多，一般我们在全连接层的后面跟Dropout操作，防止过拟合。

